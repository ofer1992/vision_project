{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using laplacian to estimate blurriness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('../lecture.mp4')\n",
    "font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "bottomLeftCornerOfText = (10,500)\n",
    "fontScale              = 1\n",
    "fontColor              = (255,255,255)\n",
    "lineType               = 2\n",
    "\n",
    "while 1:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    laplacian_var = cv2.Laplacian(frame, cv2.CV_64F).var()\n",
    "    \n",
    "    cv2.putText(frame,str(laplacian_var),\n",
    "        bottomLeftCornerOfText, \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "    cv2.imshow('vid',frame)\n",
    "    k = cv2.waitKey(20) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate homography each second when frame not blurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_FEATURES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    " \n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "def alignImages(im1, im2):\n",
    "    factor = 4.\n",
    "    small1 = cv2.resize(im1, (0,0), fx=1/factor, fy=1/factor) \n",
    "    small2 = cv2.resize(im2, (0,0), fx=1/factor, fy=1/factor)\n",
    "    \n",
    "    rects1, weights1 = hog.detectMultiScale(small1, winStride=(4, 4),\n",
    "        padding=(8, 8), scale=1.05)\n",
    "    rects2, weights2 = hog.detectMultiScale(small2, winStride=(4, 4),\n",
    "        padding=(8, 8), scale=1.05)\n",
    "    rects1 = [[int(i*factor) for i in r] for r in rects1]\n",
    "    rects2 = [[int(i*factor) for i in r] for r in rects2]\n",
    "    mask1 = np.ones(im1.shape[:2],dtype='uint8') * 255\n",
    "    mask2 = np.ones(im2.shape[:2],dtype='uint8') * 255\n",
    "    for (x, y, w, h) in rects1:\n",
    "\t\tcv2.rectangle(mask1, (x-w, y), (x + w*2, y + h), 0, -1)\n",
    "    for (x, y, w, h) in rects2:\n",
    "\t\tcv2.rectangle(mask2, (x-w, y), (x + w*2, y + h), 0, -1)\n",
    "    # Convert images to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, mask1)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, mask2)\n",
    "\n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "\n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    "\n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "\n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        x1,y1 = keypoints1[match.queryIdx].pt\n",
    "        x2,y2 = keypoints2[match.trainIdx].pt\n",
    "        if abs(y1 - y2) >= 100:\n",
    "            continue\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "\n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "\n",
    "    # reprojection error calculation\n",
    "    points1t = np.hstack((points1,np.ones((points1.shape[0], 1))))\n",
    "    homo_vec = (np.dot(h, points1t.T)).T\n",
    "    dehomo_vec = homo_vec[:,:2] / homo_vec[:,2:3]\n",
    "    error = np.linalg.norm(dehomo_vec-points2, axis=1).mean()\n",
    "    \n",
    "    # Use homography\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "    return im1Reg, h, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n",
      "skipped\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('../lecture.mp4')\n",
    "_, old_frame = cap.read()\n",
    "\n",
    "while 1:\n",
    "    for i in range(10):\n",
    "        if not cap.grab():\n",
    "            break # TODO: only breaks from inner loop...\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    laplacian_var = cv2.Laplacian(frame, cv2.CV_64F).var()\n",
    "    if laplacian_var < 450.:\n",
    "        print \"skipped\"\n",
    "        continue\n",
    "    aligned, h, error = alignImages(old_frame, frame)\n",
    "#     sing_val = np.linalg.svd(h)[1]\n",
    "#     str(sing_val[0]) + \" \" + str(sing_val[0]/sing_val[2]))\n",
    "    blended = cv2.addWeighted(frame,.5,aligned,.5,0)\n",
    "    cv2.putText(blended,str(error),\n",
    "        bottomLeftCornerOfText, \n",
    "        font, \n",
    "        fontScale,\n",
    "        fontColor,\n",
    "        lineType)\n",
    "    cv2.imshow('aligned', blended)\n",
    "    if error < 10:\n",
    "        cv2.imshow('good', frame)\n",
    "        old_frame = frame\n",
    "    k = cv2.waitKey(10) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
